{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "news_sentiment_lstm_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11n-QmRqlJyajqhQprHcIxi1RVTMGWBfp",
      "authorship_tag": "ABX9TyMqlF2SM+idKycRJOXjBc/K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zqiaohe/smi_tags/blob/main/news_sentiment_lstm_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtAEi6psZ-is"
      },
      "source": [
        "#BASELINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdJTvsbuU0il"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33NVArMnWAez",
        "outputId": "18cf640a-9b60-4df0-c10f-cc971e6b8937"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4q-BHHoVAZ0"
      },
      "source": [
        "with open('/content/drive/MyDrive/FH_data/sentiment-analysis-in-russian/train.json') as f:\n",
        "    raw_train = json.load(f)\n",
        "with open('/content/drive/MyDrive/FH_data/sentiment-analysis-in-russian/test.json') as f:\n",
        "    raw_test = json.load(f)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_aI4F69V267"
      },
      "source": [
        "def ru_token(string):\n",
        "    \"\"\"russian tokenize based on nltk.word_tokenize. only russian letter remaind.\"\"\"\n",
        "    return [i for i in word_tokenize(string) if re.match(r'[\\u0400-\\u04ffа́]+$', i)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbIlNxmjV7H0"
      },
      "source": [
        "params = {}\n",
        "params['tokenizer'] = ru_token\n",
        "params['stop_words'] = stopwords.words('russian')\n",
        "params['ngram_range'] = (1, 3)\n",
        "params['min_df'] = 3"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSM8OX55WEEV"
      },
      "source": [
        "tfidf  = TfidfVectorizer(**params)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNqZeeUrWGZ1",
        "outputId": "ff5479dd-10dd-42a4-cec2-e63f0c7c924f"
      },
      "source": [
        "tfidf.fit([i['text'] for i in raw_train + raw_test])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=3, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True,\n",
              "                stop_words=['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с',\n",
              "                            'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его',\n",
              "                            'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы',\n",
              "                            'по', 'только', 'ее', 'мне', ...],\n",
              "                strip_accents=None, sublinear_tf=False,\n",
              "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=<function ru_token at 0x7f584d4fa440>, use_idf=True,\n",
              "                vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3-FQNrhWPql"
      },
      "source": [
        "train = {}\n",
        "val = {}\n",
        "tmp = defaultdict(list)\n",
        "for e in raw_train:\n",
        "    tmp[e['sentiment']].append(e['text'])\n",
        "for l in tmp:\n",
        "    train[l], val[l] = train_test_split(tmp[l], test_size=0.2, random_state=2018)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuoreM_nWRL7"
      },
      "source": [
        "def upsampling_align(some_dict, random_state=2018):\n",
        "    rand = np.random.RandomState(random_state)\n",
        "    upper = max([len(some_dict[l]) for l in some_dict])\n",
        "    print('upper bound: {}'.format(upper))\n",
        "    tmp = {}\n",
        "    for l in some_dict:\n",
        "        if len(some_dict[l]) < upper:\n",
        "            repeat_time = int(upper/len(some_dict[l]))\n",
        "            remainder = upper % len(some_dict[l])\n",
        "            _tmp = some_dict[l].copy()\n",
        "            rand.shuffle(_tmp)\n",
        "            tmp[l] = some_dict[l] * repeat_time + _tmp[:remainder]\n",
        "            rand.shuffle(tmp[l])\n",
        "        else:\n",
        "            tmp[l] = some_dict[l]\n",
        "    return tmp"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofzuZYDxWa2N",
        "outputId": "271b9893-a0de-474c-efe4-3a742aa2242a"
      },
      "source": [
        "btrain = upsampling_align(train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "upper bound: 3227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_Sleiv6Wbkj"
      },
      "source": [
        "m_params = {}\n",
        "m_params['solver'] = 'lbfgs'\n",
        "m_params['multi_class'] = 'multinomial'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kzxzMVKWfif"
      },
      "source": [
        "softmax = LogisticRegression(**m_params)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "Jpd8-OohWidT",
        "outputId": "b377575b-3b4f-44d5-b7f8-30f9d2e4889b"
      },
      "source": [
        "train_x = [j for i in sorted(btrain.keys()) for j in btrain[i]]\n",
        "train_y = [i for i in sorted(btrain.keys()) for j in btrain[i]]\n",
        "softmax.fit(tfidf.transform(train_x), train_y)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-31b798aeec36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msoftmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                    \"be removed in 0.24.\")\n\u001b[1;32m   1897\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-fc286fa6fd13>\u001b[0m in \u001b[0;36mru_token\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mru_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"russian tokenize based on nltk.word_tokenize. only russian letter remaind.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[\\u0400-\\u04ffа́]+$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'next_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtext_contains_sentbreak\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \"\"\"\n\u001b[1;32m   1336\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m# used to ignore last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_annotate_second_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfrequent\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mstarter\u001b[0m \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \"\"\"\n\u001b[0;32m-> 1472\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_second_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_annotate_first_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    579\u001b[0m           \u001b[0;34m-\u001b[0m \u001b[0mellipsis_toks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mindices\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mellipsis\u001b[0m \u001b[0mmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \"\"\"\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0maug_tok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0maug_tok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_tokenize_words\u001b[0;34m(self, plaintext)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m                 yield self._Token(next(line_toks),\n\u001b[0;32m--> 551\u001b[0;31m                         parastart=parastart, linestart=True)\n\u001b[0m\u001b[1;32m    552\u001b[0m                 \u001b[0mparastart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tok, **params)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_get_type\u001b[0;34m(self, tok)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;34m\"\"\"Returns a case-normalized representation of the token.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RE_NUMERIC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'##number##'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDMuAK6iWkLO"
      },
      "source": [
        "test_x = [j for i in sorted(val.keys()) for j in val[i]]\n",
        "true = [i for i in sorted(val.keys()) for j in val[i]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQmDY0H4WmAW"
      },
      "source": [
        "pred = softmax.predict(tfidf.transform(test_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Vs5AkKWtYn"
      },
      "source": [
        "lab = LabelEncoder()\n",
        "c_true = lab.fit_transform(true)\n",
        "c_pred = lab.transform(pred)\n",
        "print(classification_report(c_true, c_pred, target_names=lab.classes_, digits=5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbyP28uKWvZz"
      },
      "source": [
        "bval = upsampling_align(val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9SxIKJ3WxmE"
      },
      "source": [
        "b_test_x = [j for i in sorted(bval.keys()) for j in bval[i]]\n",
        "b_true = [i for i in sorted(bval.keys()) for j in bval[i]]\n",
        "b_pred = softmax.predict(tfidf.transform(b_test_x))\n",
        "lab = LabelEncoder()\n",
        "c_true = lab.fit_transform(b_true)\n",
        "c_pred = lab.transform(b_pred)\n",
        "print(classification_report(c_true, c_pred, target_names=lab.classes_, digits=5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NreFf76kWz92"
      },
      "source": [
        "sub_pred = softmax.predict(tfidf.transform([i['text'] for i in raw_test]))\n",
        "sub_df = pd.DataFrame()\n",
        "sub_df['id'] =  [i['id'] for i in raw_test]\n",
        "sub_df['sentiment'] = sub_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJRpWqwUW0aD"
      },
      "source": [
        "sub_df.to_csv('softmax_reg.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR7-4WWeaE4F"
      },
      "source": [
        "#МОЯ ВЕРСИЯ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnnAqqdbaM0k"
      },
      "source": [
        "##Обработка данных — преобразование в нижний регистр"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "8nrMrPyyX1He",
        "outputId": "4df1130e-51b1-4c74-8fbc-b53731224302"
      },
      "source": [
        "df = pd.read_json('/content/drive/MyDrive/FH_data/sentiment-analysis-in-russian/train.json')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-99e8d6988704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/FH_data/sentiment-analysis-in-russian/train.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMir0nAFZjnM"
      },
      "source": [
        "df['text_lower'] = [x.lower() for x in df['text']]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHT9hWnH4t-C"
      },
      "source": [
        "testdf = pd.read_json('/content/drive/MyDrive/FH_data/sentiment-analysis-in-russian/train.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0CfdkPB5Jrr"
      },
      "source": [
        "header = ['id', 'pubdate', 'pubtime', 'title', 'tag', 'theme', 'null']\n",
        "baikaldf = pd.read_csv('/content/drive/MyDrive/FH_data/baikaldaily_all_sorted.csv', sep =';', error_bad_lines=False, names=header) \n",
        "baikaldf = baikaldf.drop(columns=['null'])\n",
        "baikaldf = baikaldf[baikaldf['pubdate']!='None']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_vwz16Eadvu"
      },
      "source": [
        "##Обработка данных — удаление знаков препинания и токенизация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlp3n-kraff2"
      },
      "source": [
        "from string import punctuation"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZE6ok94WceX7",
        "outputId": "59c54d3b-1cc0-4bae-e095-f7a3a63ba706"
      },
      "source": [
        "punctuation"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB7J2Qm4cs40"
      },
      "source": [
        "punctuation = punctuation + '\\n' + '»' + '«'"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mlj88Le7amwu",
        "outputId": "a15af5cf-97a7-44b4-ae80-a3b19b040c9d"
      },
      "source": [
        "for i in range(len(df['text_lower'])):\n",
        "  df['text_lower'][i] =  ''.join([c for c in df['text_lower'][i] if c not in punctuation]).split(' ')"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bad4j4LGcXp-",
        "outputId": "8e0d92d4-1ce8-44ec-94a2-d2a5704c033f"
      },
      "source": [
        " df['text_lower']"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [досудебное, расследование, по, факту, покупки...\n",
              "1       [медики, рассказали, о, состоянии, пострадавше...\n",
              "2       [прошел, почти, год, как, железнодорожным, опе...\n",
              "3       [по, итогам, 12, месяцев, 2016, года, на, терр...\n",
              "4       [астана, 21, ноября, kazakhstan, today, , аген...\n",
              "                              ...                        \n",
              "8258    [как, мы, писали, еще, весной, для, увеличения...\n",
              "8259    [но, молодой, министр, национальной, экономики...\n",
              "8260    [, в, енпф, назначен, новый, председатель, пра...\n",
              "8261    [в, алматы, у, отделения, банка, произошло, на...\n",
              "8262    [нпп, рк, атамекен, предлагает, создать, нацио...\n",
              "Name: text_lower, Length: 8263, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVkQXW_3dvaO"
      },
      "source": [
        "##Токенизация — Создание словаря сопоставления Vocab to Int"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f91d8rKWhe9c"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umTJXk7GdRfT"
      },
      "source": [
        "all_texts = []\n",
        "for i in range(len(df['text_lower'])):\n",
        "  all_texts.append(' '.join(df['text_lower'][i]))\n",
        "all_texts = ' '.join(all_texts)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utyqydp1g3yk"
      },
      "source": [
        "words = all_texts.split(' ')"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq4bVdMVhc9p"
      },
      "source": [
        "count_words = Counter(words)\n",
        "total_words = len(words)\n",
        "sorted_words = count_words.most_common(total_words)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tqDvK9Shpaz"
      },
      "source": [
        "vocab_to_int = {w:i for i, (w,c) in enumerate(sorted_words)}"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcrUzRqGhyh4"
      },
      "source": [
        "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY0FlB3Gos_k",
        "outputId": "ffadb73f-8d17-457c-c9ad-f8219c3e40b5"
      },
      "source": [
        "vocab_to_int"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'в': 1,\n",
              " 'и': 2,\n",
              " '': 3,\n",
              " 'на': 4,\n",
              " 'по': 5,\n",
              " 'с': 6,\n",
              " 'что': 7,\n",
              " 'не': 8,\n",
              " 'года': 9,\n",
              " 'для': 10,\n",
              " 'за': 11,\n",
              " 'к': 12,\n",
              " '–': 13,\n",
              " 'а': 14,\n",
              " 'из': 15,\n",
              " '—': 16,\n",
              " 'тенге': 17,\n",
              " 'о': 18,\n",
              " 'это': 19,\n",
              " 'как': 20,\n",
              " 'до': 21,\n",
              " 'от': 22,\n",
              " 'году': 23,\n",
              " 'также': 24,\n",
              " 'казахстана': 25,\n",
              " 'мы': 26,\n",
              " 'ао': 27,\n",
              " 'казахстан': 28,\n",
              " '2016': 29,\n",
              " 'будет': 30,\n",
              " 'рк': 31,\n",
              " 'при': 32,\n",
              " 'но': 33,\n",
              " 'области': 34,\n",
              " 'все': 35,\n",
              " 'его': 36,\n",
              " 'республики': 37,\n",
              " 'млрд': 38,\n",
              " 'этом': 39,\n",
              " 'у': 40,\n",
              " '1': 41,\n",
              " 'он': 42,\n",
              " 'то': 43,\n",
              " 'более': 44,\n",
              " 'развития': 45,\n",
              " 'лет': 46,\n",
              " 'их': 47,\n",
              " 'или': 48,\n",
              " 'так': 49,\n",
              " 'было': 50,\n",
              " 'уже': 51,\n",
              " 'том': 52,\n",
              " 'время': 53,\n",
              " 'которые': 54,\n",
              " 'был': 55,\n",
              " 'только': 56,\n",
              " 'есть': 57,\n",
              " 'же': 58,\n",
              " 'будут': 59,\n",
              " '2015': 60,\n",
              " 'алматы': 61,\n",
              " 'казахстане': 62,\n",
              " 'год': 63,\n",
              " 'я': 64,\n",
              " 'тоо': 65,\n",
              " 'млн': 66,\n",
              " 'страны': 67,\n",
              " 'компании': 68,\n",
              " 'банка': 69,\n",
              " 'были': 70,\n",
              " 'со': 71,\n",
              " 'того': 72,\n",
              " 'экономики': 73,\n",
              " 'является': 74,\n",
              " 'рамках': 75,\n",
              " 'об': 76,\n",
              " 'енпф': 77,\n",
              " 'если': 78,\n",
              " 'назад': 79,\n",
              " '2017': 80,\n",
              " 'государства': 81,\n",
              " 'еще': 82,\n",
              " 'они': 83,\n",
              " 'банк': 84,\n",
              " '2': 85,\n",
              " 'сегодня': 86,\n",
              " 'этого': 87,\n",
              " 'тыс': 88,\n",
              " '10': 89,\n",
              " 'который': 90,\n",
              " 'всего': 91,\n",
              " 'чтобы': 92,\n",
              " 'после': 93,\n",
              " 'января': 94,\n",
              " 'во': 95,\n",
              " 'средств': 96,\n",
              " 'г': 97,\n",
              " 'между': 98,\n",
              " 'национальной': 99,\n",
              " 'всех': 100,\n",
              " 'тысяч': 101,\n",
              " 'ее': 102,\n",
              " 'управления': 103,\n",
              " 'президента': 104,\n",
              " 'сейчас': 105,\n",
              " 'необходимо': 106,\n",
              " 'астана': 107,\n",
              " 'может': 108,\n",
              " 'под': 109,\n",
              " 'тем': 110,\n",
              " '3': 111,\n",
              " 'работы': 112,\n",
              " 'через': 113,\n",
              " 'декабря': 114,\n",
              " 'заместитель': 115,\n",
              " 'директор': 116,\n",
              " '5': 117,\n",
              " 'компания': 118,\n",
              " 'отметил': 119,\n",
              " 'нас': 120,\n",
              " 'числе': 121,\n",
              " 'бы': 122,\n",
              " 'фонда': 123,\n",
              " 'когда': 124,\n",
              " 'быть': 125,\n",
              " 'им': 126,\n",
              " 'них': 127,\n",
              " 'была': 128,\n",
              " 'реализации': 129,\n",
              " 'можно': 130,\n",
              " 'нужно': 131,\n",
              " 'проектов': 132,\n",
              " 'программы': 133,\n",
              " 'очень': 134,\n",
              " 'чем': 135,\n",
              " 'деятельности': 136,\n",
              " 'эти': 137,\n",
              " 'правления': 138,\n",
              " 'автомобилей': 139,\n",
              " 'новых': 140,\n",
              " 'связи': 141,\n",
              " 'этот': 142,\n",
              " 'глава': 143,\n",
              " 'этой': 144,\n",
              " 'президент': 145,\n",
              " 'развитию': 146,\n",
              " 'производства': 147,\n",
              " '4': 148,\n",
              " 'города': 149,\n",
              " 'министерства': 150,\n",
              " 'председатель': 151,\n",
              " 'день': 152,\n",
              " 'стран': 153,\n",
              " 'ходе': 154,\n",
              " 'она': 155,\n",
              " 'национального': 156,\n",
              " 'годы': 157,\n",
              " 'словам': 158,\n",
              " 'проекта': 159,\n",
              " 'без': 160,\n",
              " 'россии': 161,\n",
              " 'кроме': 162,\n",
              " 'компаний': 163,\n",
              " 'комитета': 164,\n",
              " 'счет': 165,\n",
              " 'департамента': 166,\n",
              " 'сказал': 167,\n",
              " 'бизнеса': 168,\n",
              " 'где': 169,\n",
              " 'председателя': 170,\n",
              " 'человек': 171,\n",
              " 'должны': 172,\n",
              " 'вопрос': 173,\n",
              " 'итогам': 174,\n",
              " 'тонн': 175,\n",
              " 'продукции': 176,\n",
              " 'которых': 177,\n",
              " 'безопасности': 178,\n",
              " 'долларов': 179,\n",
              " '20': 180,\n",
              " '15': 181,\n",
              " 'правительства': 182,\n",
              " 'один': 183,\n",
              " 'нет': 184,\n",
              " 'назарбаев': 185,\n",
              " 'информации': 186,\n",
              " 'сумму': 187,\n",
              " 'средства': 188,\n",
              " 'однако': 189,\n",
              " 'ноября': 190,\n",
              " 'совета': 191,\n",
              " 'больше': 192,\n",
              " 'ранее': 193,\n",
              " 'населения': 194,\n",
              " 'два': 195,\n",
              " 'участие': 196,\n",
              " 'развитие': 197,\n",
              " 'министр': 198,\n",
              " '2014': 199,\n",
              " 'сша': 200,\n",
              " 'работу': 201,\n",
              " 'здесь': 202,\n",
              " 'сообщил': 203,\n",
              " 'системы': 204,\n",
              " 'которая': 205,\n",
              " 'проект': 206,\n",
              " 'рост': 207,\n",
              " 'свою': 208,\n",
              " 'поэтому': 209,\n",
              " 'строительство': 210,\n",
              " 'сфере': 211,\n",
              " '30': 212,\n",
              " 'даже': 213,\n",
              " 'передает': 214,\n",
              " 'деньги': 215,\n",
              " 'около': 216,\n",
              " '100': 217,\n",
              " 'стране': 218,\n",
              " 'могут': 219,\n",
              " 'государственных': 220,\n",
              " '12': 221,\n",
              " 'период': 222,\n",
              " '6': 223,\n",
              " 'раз': 224,\n",
              " '25': 225,\n",
              " 'других': 226,\n",
              " 'стороны': 227,\n",
              " 'астане': 228,\n",
              " 'объем': 229,\n",
              " 'уровне': 230,\n",
              " 'согласно': 231,\n",
              " 'активов': 232,\n",
              " 'месяцев': 233,\n",
              " 'ли': 234,\n",
              " 'первый': 235,\n",
              " 'государственной': 236,\n",
              " 'рынка': 237,\n",
              " '7': 238,\n",
              " 'вы': 239,\n",
              " 'ст': 240,\n",
              " 'фонд': 241,\n",
              " 'уровня': 242,\n",
              " 'пенсионных': 243,\n",
              " 'составляет': 244,\n",
              " 'инвестиций': 245,\n",
              " 'размере': 246,\n",
              " 'банков': 247,\n",
              " '9': 248,\n",
              " 'рынке': 249,\n",
              " 'среди': 250,\n",
              " 'сообщает': 251,\n",
              " 'кто': 252,\n",
              " 'астаны': 253,\n",
              " 'вопросы': 254,\n",
              " 'нам': 255,\n",
              " '11': 256,\n",
              " 'случае': 257,\n",
              " 'свои': 258,\n",
              " 'главы': 259,\n",
              " '8': 260,\n",
              " 'октября': 261,\n",
              " 'суда': 262,\n",
              " 'данным': 263,\n",
              " 'роста': 264,\n",
              " 'решение': 265,\n",
              " 'предприятий': 266,\n",
              " 'этих': 267,\n",
              " 'соответствии': 268,\n",
              " 'услуг': 269,\n",
              " 'детей': 270,\n",
              " 'территории': 271,\n",
              " 'порядка': 272,\n",
              " 'должен': 273,\n",
              " 'новые': 274,\n",
              " 'организации': 275,\n",
              " 'составил': 276,\n",
              " 'лиц': 277,\n",
              " 'стал': 278,\n",
              " 'уровень': 279,\n",
              " 'надо': 280,\n",
              " 'доля': 281,\n",
              " 'другие': 282,\n",
              " '16': 283,\n",
              " 'отношении': 284,\n",
              " 'сотрудничества': 285,\n",
              " 'тогда': 286,\n",
              " 'образования': 287,\n",
              " '000': 288,\n",
              " 'казахстанской': 289,\n",
              " 'инвестициям': 290,\n",
              " 'акима': 291,\n",
              " 'ни': 292,\n",
              " 'байтерек': 293,\n",
              " '14': 294,\n",
              " 'именно': 295,\n",
              " 'часть': 296,\n",
              " 'государственного': 297,\n",
              " 'должности': 298,\n",
              " 'авто': 299,\n",
              " 'настоящее': 300,\n",
              " 'целом': 301,\n",
              " 'строительства': 302,\n",
              " 'национальный': 303,\n",
              " 'очередь': 304,\n",
              " 'отрасли': 305,\n",
              " 'центр': 306,\n",
              " 'предпринимателей': 307,\n",
              " 'место': 308,\n",
              " 'казахстанских': 309,\n",
              " 'нового': 310,\n",
              " 'своей': 311,\n",
              " 'несколько': 312,\n",
              " 'новый': 313,\n",
              " 'возможность': 314,\n",
              " 'нурсултан': 315,\n",
              " 'планируется': 316,\n",
              " 'работа': 317,\n",
              " 'поддержки': 318,\n",
              " '50': 319,\n",
              " 'начала': 320,\n",
              " 'которой': 321,\n",
              " 'теперь': 322,\n",
              " 'пока': 323,\n",
              " 'руководитель': 324,\n",
              " 'себя': 325,\n",
              " 'предприятия': 326,\n",
              " 'совместно': 327,\n",
              " 'миллионов': 328,\n",
              " 'нашей': 329,\n",
              " 'деятельность': 330,\n",
              " 'начальник': 331,\n",
              " 'раза': 332,\n",
              " 'всем': 333,\n",
              " 'инфраструктуры': 334,\n",
              " 'промышленности': 335,\n",
              " 'двух': 336,\n",
              " 'директора': 337,\n",
              " 'три': 338,\n",
              " 'производство': 339,\n",
              " 'стало': 340,\n",
              " 'граждан': 341,\n",
              " 'перед': 342,\n",
              " 'рынок': 343,\n",
              " 'части': 344,\n",
              " 'обеспечить': 345,\n",
              " 'количество': 346,\n",
              " 'нефти': 347,\n",
              " 'отдела': 348,\n",
              " 'меры': 349,\n",
              " 'которого': 350,\n",
              " 'экономического': 351,\n",
              " '21': 352,\n",
              " 'сайта': 353,\n",
              " 'имеет': 354,\n",
              " 'центра': 355,\n",
              " 'процентов': 356,\n",
              " 'внимание': 357,\n",
              " 'иностранных': 358,\n",
              " 'стоимость': 359,\n",
              " 'этим': 360,\n",
              " 'объектов': 361,\n",
              " 'течение': 362,\n",
              " 'казахстанского': 363,\n",
              " 'следует': 364,\n",
              " '№': 365,\n",
              " 'новой': 366,\n",
              " 'статьи': 367,\n",
              " 'такие': 368,\n",
              " 'образом': 369,\n",
              " 'результате': 370,\n",
              " 'вместе': 371,\n",
              " 'союза': 372,\n",
              " 'политики': 373,\n",
              " 'правительству': 374,\n",
              " 'менее': 375,\n",
              " 'предпринимательства': 376,\n",
              " 'сектора': 377,\n",
              " 'дел': 378,\n",
              " 'района': 379,\n",
              " 'министра': 380,\n",
              " '2013': 381,\n",
              " 'потому': 382,\n",
              " '13': 383,\n",
              " 'текущего': 384,\n",
              " 'условиях': 385,\n",
              " 'момент': 386,\n",
              " 'изза': 387,\n",
              " 'работе': 388,\n",
              " '2020': 389,\n",
              " 'являются': 390,\n",
              " 'сентября': 391,\n",
              " 'наших': 392,\n",
              " 'почти': 393,\n",
              " 'нацбанка': 394,\n",
              " 'таких': 395,\n",
              " 'решения': 396,\n",
              " 'рассказал': 397,\n",
              " 'там': 398,\n",
              " 'лишь': 399,\n",
              " 'мира': 400,\n",
              " 'качестве': 401,\n",
              " 'заявил': 402,\n",
              " 'еаэс': 403,\n",
              " 'подчеркнул': 404,\n",
              " 'вот': 405,\n",
              " 'просто': 406,\n",
              " 'сумма': 407,\n",
              " 'своих': 408,\n",
              " 'такой': 409,\n",
              " 'первого': 410,\n",
              " 'обеспечения': 411,\n",
              " 'вопросам': 412,\n",
              " 'родился': 413,\n",
              " 'второй': 414,\n",
              " 'независимости': 415,\n",
              " 'аким': 416,\n",
              " 'службы': 417,\n",
              " 'сообщили': 418,\n",
              " 'одного': 419,\n",
              " 'говорится': 420,\n",
              " 'стать': 421,\n",
              " 'международных': 422,\n",
              " 'таким': 423,\n",
              " 'суд': 424,\n",
              " 'эта': 425,\n",
              " 'кредитов': 426,\n",
              " 'частности': 427,\n",
              " 'либо': 428,\n",
              " 'lada': 429,\n",
              " 'мест': 430,\n",
              " 'людей': 431,\n",
              " 'одним': 432,\n",
              " '40': 433,\n",
              " 'дело': 434,\n",
              " 'например': 435,\n",
              " 'проекты': 436,\n",
              " 'идет': 437,\n",
              " 'бишимбаев': 438,\n",
              " 'бюджета': 439,\n",
              " 'ряд': 440,\n",
              " 'финансовых': 441,\n",
              " 'правительство': 442,\n",
              " 'стали': 443,\n",
              " 'экономической': 444,\n",
              " 'наиболее': 445,\n",
              " 'денег': 446,\n",
              " 'миллиардов': 447,\n",
              " 'должно': 448,\n",
              " '17': 449,\n",
              " 'представители': 450,\n",
              " 'назарбаева': 451,\n",
              " '18': 452,\n",
              " 'органов': 453,\n",
              " 'создание': 454,\n",
              " 'главный': 455,\n",
              " 'директоров': 456,\n",
              " 'университет': 457,\n",
              " 'последние': 458,\n",
              " 'сообщению': 459,\n",
              " 'региона': 460,\n",
              " 'актау': 461,\n",
              " 'нк': 462,\n",
              " 'прессслужба': 463,\n",
              " 'государство': 464,\n",
              " 'известно': 465,\n",
              " 'технологий': 466,\n",
              " 'акций': 467,\n",
              " 'составила': 468,\n",
              " 'финансов': 469,\n",
              " 'товаров': 470,\n",
              " 'эту': 471,\n",
              " 'власти': 472,\n",
              " 'мне': 473,\n",
              " 'позволит': 474,\n",
              " 'завод': 475,\n",
              " 'окончил': 476,\n",
              " 'стоит': 477,\n",
              " 'тоже': 478,\n",
              " 'те': 479,\n",
              " 'условия': 480,\n",
              " 'много': 481,\n",
              " 'транспорта': 482,\n",
              " 'права': 483,\n",
              " 'ему': 484,\n",
              " 'система': 485,\n",
              " 'одной': 486,\n",
              " 'участием': 487,\n",
              " 'международного': 488,\n",
              " 'мире': 489,\n",
              " '2012': 490,\n",
              " 'бизнес': 491,\n",
              " 'область': 492,\n",
              " 'важно': 493,\n",
              " 'каждый': 494,\n",
              " 'полностью': 495,\n",
              " 'программе': 496,\n",
              " 'институт': 497,\n",
              " 'времени': 498,\n",
              " 'экспо2017': 499,\n",
              " 'контроля': 500,\n",
              " '26': 501,\n",
              " 'напомним': 502,\n",
              " 'сравнению': 503,\n",
              " 'парламента': 504,\n",
              " 'жизни': 505,\n",
              " 'благодаря': 506,\n",
              " 'считает': 507,\n",
              " 'возможности': 508,\n",
              " 'проведения': 509,\n",
              " 'энергетики': 510,\n",
              " 'группы': 511,\n",
              " 'свой': 512,\n",
              " 'работать': 513,\n",
              " 'конце': 514,\n",
              " 'город': 515,\n",
              " 'виде': 516,\n",
              " 'дня': 517,\n",
              " 'корреспондент': 518,\n",
              " 'государственный': 519,\n",
              " 'организаций': 520,\n",
              " 'помощи': 521,\n",
              " 'поскольку': 522,\n",
              " 'работ': 523,\n",
              " 'говорит': 524,\n",
              " '2011': 525,\n",
              " 'создать': 526,\n",
              " 'сми': 527,\n",
              " 'мнению': 528,\n",
              " 'международной': 529,\n",
              " 'прошлого': 530,\n",
              " 'казинформ': 531,\n",
              " 'многие': 532,\n",
              " 'цен': 533,\n",
              " 'данный': 534,\n",
              " 'целях': 535,\n",
              " 'люди': 536,\n",
              " 'основе': 537,\n",
              " 'своего': 538,\n",
              " 'инвесторов': 539,\n",
              " 'назначен': 540,\n",
              " 'акции': 541,\n",
              " 'завода': 542,\n",
              " 'конца': 543,\n",
              " 'пути': 544,\n",
              " 'модернизации': 545,\n",
              " 'свыше': 546,\n",
              " 'цены': 547,\n",
              " 'конечно': 548,\n",
              " '23': 549,\n",
              " 'областного': 550,\n",
              " 'дела': 551,\n",
              " '28': 552,\n",
              " 'получить': 553,\n",
              " 'крупных': 554,\n",
              " 'активно': 555,\n",
              " 'трлн': 556,\n",
              " 'февраля': 557,\n",
              " 'агентства': 558,\n",
              " 'здравоохранения': 559,\n",
              " 'поддержку': 560,\n",
              " 'хозяйства': 561,\n",
              " '19': 562,\n",
              " '200': 563,\n",
              " 'финансирования': 564,\n",
              " 'сегодняшний': 565,\n",
              " 'должна': 566,\n",
              " 'снижение': 567,\n",
              " 'рабочих': 568,\n",
              " 'достаточно': 569,\n",
              " 'курс': 570,\n",
              " 'хотя': 571,\n",
              " 'особенно': 572,\n",
              " '24': 573,\n",
              " 'заседания': 574,\n",
              " 'прав': 575,\n",
              " 'начале': 576,\n",
              " 'руководителя': 577,\n",
              " 'ситуации': 578,\n",
              " 'труда': 579,\n",
              " 'комиссии': 580,\n",
              " 'программа': 581,\n",
              " 'участников': 582,\n",
              " 'ставки': 583,\n",
              " '2010': 584,\n",
              " 'капитала': 585,\n",
              " 'против': 586,\n",
              " 'срок': 587,\n",
              " 'него': 588,\n",
              " 'системе': 589,\n",
              " 'нефть': 590,\n",
              " 'ссылкой': 591,\n",
              " 'июля': 592,\n",
              " '2018': 593,\n",
              " 'км': 594,\n",
              " 'целью': 595,\n",
              " 'тот': 596,\n",
              " '500': 597,\n",
              " 'сети': 598,\n",
              " 'меня': 599,\n",
              " 'городе': 600,\n",
              " 'республике': 601,\n",
              " 'первую': 602,\n",
              " 'банки': 603,\n",
              " 'ситуация': 604,\n",
              " 'опыт': 605,\n",
              " 'этому': 606,\n",
              " 'участия': 607,\n",
              " 'единиц': 608,\n",
              " 'проблемы': 609,\n",
              " 'почему': 610,\n",
              " 'всегда': 611,\n",
              " 'учетом': 612,\n",
              " 'затем': 613,\n",
              " 'банком': 614,\n",
              " 'несмотря': 615,\n",
              " 'некоторые': 616,\n",
              " 'услуги': 617,\n",
              " 'далее': 618,\n",
              " 'большой': 619,\n",
              " 'источник': 620,\n",
              " 'данные': 621,\n",
              " 'изменения': 622,\n",
              " 'субъектов': 623,\n",
              " 'ндс': 624,\n",
              " 'социальной': 625,\n",
              " 'ведь': 626,\n",
              " 'казахстанцев': 627,\n",
              " 'тех': 628,\n",
              " 'комплекса': 629,\n",
              " 'заседании': 630,\n",
              " 'свое': 631,\n",
              " 'сразу': 632,\n",
              " 'ед': 633,\n",
              " 'социальных': 634,\n",
              " 'финансовой': 635,\n",
              " 'выше': 636,\n",
              " 'закона': 637,\n",
              " 'которое': 638,\n",
              " 'жол': 639,\n",
              " 'управление': 640,\n",
              " 'наши': 641,\n",
              " 'китай': 642,\n",
              " 'практически': 643,\n",
              " '29': 644,\n",
              " 'отметить': 645,\n",
              " 'пенсионные': 646,\n",
              " 'создания': 647,\n",
              " 'получения': 648,\n",
              " 'дома': 649,\n",
              " 'kazakhstan': 650,\n",
              " 'комплекс': 651,\n",
              " 'қазақстан': 652,\n",
              " '22': 653,\n",
              " 'дней': 654,\n",
              " 'путь': 655,\n",
              " 'вагонов': 656,\n",
              " 'экономике': 657,\n",
              " 'мажилиса': 658,\n",
              " 'сделки': 659,\n",
              " 'имущества': 660,\n",
              " 'института': 661,\n",
              " 'перевозок': 662,\n",
              " 'помощь': 663,\n",
              " 'будем': 664,\n",
              " 'информацию': 665,\n",
              " 'дальше': 666,\n",
              " 'пассажиров': 667,\n",
              " 'суммы': 668,\n",
              " 'сайте': 669,\n",
              " 'внутренних': 670,\n",
              " 'состав': 671,\n",
              " '31': 672,\n",
              " 'данных': 673,\n",
              " 'находится': 674,\n",
              " 'над': 675,\n",
              " 'республиканского': 676,\n",
              " 'странах': 677,\n",
              " 'систему': 678,\n",
              " 'принять': 679,\n",
              " 'ввп': 680,\n",
              " 'самрукказына': 681,\n",
              " 'общества': 682,\n",
              " 'прошлом': 683,\n",
              " 'культуры': 684,\n",
              " 'нации': 685,\n",
              " 'повышение': 686,\n",
              " 'работал': 687,\n",
              " 'реализация': 688,\n",
              " '60': 689,\n",
              " 'сообщении': 690,\n",
              " 'премьерминистра': 691,\n",
              " 'bank': 692,\n",
              " 'должность': 693,\n",
              " 'университета': 694,\n",
              " 'стала': 695,\n",
              " 'ктж': 696,\n",
              " 'встречи': 697,\n",
              " 'коррупции': 698,\n",
              " 'защиты': 699,\n",
              " 'первой': 700,\n",
              " 'торговли': 701,\n",
              " 'возможно': 702,\n",
              " 'пять': 703,\n",
              " 'агентство': 704,\n",
              " 'фото': 705,\n",
              " 'имени': 706,\n",
              " 'прежде': 707,\n",
              " 'качества': 708,\n",
              " 'лица': 709,\n",
              " 'спорта': 710,\n",
              " 'мер': 711,\n",
              " 'сообщила': 712,\n",
              " 'программ': 713,\n",
              " 'заместителем': 714,\n",
              " 'карагандинской': 715,\n",
              " 'кодекса': 716,\n",
              " 'план': 717,\n",
              " 'касается': 718,\n",
              " 'сделать': 719,\n",
              " 'транспортных': 720,\n",
              " 'станции': 721,\n",
              " 'процента': 722,\n",
              " 'казахстанский': 723,\n",
              " 'поезда': 724,\n",
              " 'оборудования': 725,\n",
              " 'министерство': 726,\n",
              " 'машин': 727,\n",
              " 'самых': 728,\n",
              " 'путем': 729,\n",
              " 'месте': 730,\n",
              " 'экспорта': 731,\n",
              " 'снг': 732,\n",
              " 'другой': 733,\n",
              " 'сельского': 734,\n",
              " 'предприятие': 735,\n",
              " 'задача': 736,\n",
              " 'миллиарда': 737,\n",
              " 'государств': 738,\n",
              " 'российской': 739,\n",
              " 'актобе': 740,\n",
              " 'сам': 741,\n",
              " '2009': 742,\n",
              " 'автомобили': 743,\n",
              " '2008': 744,\n",
              " 'производству': 745,\n",
              " 'доходов': 746,\n",
              " 'начальника': 747,\n",
              " 'стоимости': 748,\n",
              " 'своем': 749,\n",
              " 'котором': 750,\n",
              " 'жилья': 751,\n",
              " 'увеличение': 752,\n",
              " 'продажи': 753,\n",
              " 'финансового': 754,\n",
              " 'базе': 755,\n",
              " 'газа': 756,\n",
              " 'впервые': 757,\n",
              " 'аурум': 758,\n",
              " 'kase': 759,\n",
              " 'китая': 760,\n",
              " 'нпп': 761,\n",
              " 'малого': 762,\n",
              " 'роль': 763,\n",
              " 'документов': 764,\n",
              " 'приняли': 765,\n",
              " 'дорог': 766,\n",
              " 'второго': 767,\n",
              " 'составит': 768,\n",
              " 'активы': 769,\n",
              " 'мероприятия': 770,\n",
              " 'соответственно': 771,\n",
              " 'получил': 772,\n",
              " 'бузгул': 773,\n",
              " 'да': 774,\n",
              " 'собственности': 775,\n",
              " 'прессслужбе': 776,\n",
              " '35': 777,\n",
              " 'условий': 778,\n",
              " 'делам': 779,\n",
              " 'накоплений': 780,\n",
              " 'invest': 781,\n",
              " 'поддержке': 782,\n",
              " 'трех': 783,\n",
              " '27': 784,\n",
              " 'наша': 785,\n",
              " 'эксплуатацию': 786,\n",
              " 'получили': 787,\n",
              " 'примеру': 788,\n",
              " 'поручаю': 789,\n",
              " 'провести': 790,\n",
              " 'модели': 791,\n",
              " 'включая': 792,\n",
              " 'заместителя': 793,\n",
              " 'повышения': 794,\n",
              " 'азии': 795,\n",
              " 'месяца': 796,\n",
              " 'среднего': 797,\n",
              " 'думаю': 798,\n",
              " 'ставка': 799,\n",
              " 'требования': 800,\n",
              " 'вопросов': 801,\n",
              " 'работает': 802,\n",
              " 'отечественных': 803,\n",
              " 'представителей': 804,\n",
              " 'кредитования': 805,\n",
              " 'тысячи': 806,\n",
              " 'текущем': 807,\n",
              " 'речь': 808,\n",
              " 'ассоциации': 809,\n",
              " 'переговоры': 810,\n",
              " 'продаж': 811,\n",
              " 'число': 812,\n",
              " 'проблем': 813,\n",
              " 'рф': 814,\n",
              " 'миллиона': 815,\n",
              " 'атамекен': 816,\n",
              " 'вкладчиков': 817,\n",
              " 'финансирование': 818,\n",
              " 'нацбанк': 819,\n",
              " 'потом': 820,\n",
              " 'августа': 821,\n",
              " 'самым': 822,\n",
              " 'национальным': 823,\n",
              " 'информация': 824,\n",
              " 'законодательства': 825,\n",
              " 'администрации': 826,\n",
              " 'партии': 827,\n",
              " 'нурсултана': 828,\n",
              " 'станет': 829,\n",
              " 'науки': 830,\n",
              " 'тому': 831,\n",
              " 'долю': 832,\n",
              " 'всей': 833,\n",
              " 'технологии': 834,\n",
              " 'которую': 835,\n",
              " 'цена': 836,\n",
              " 'проводится': 837,\n",
              " 'крупнейших': 838,\n",
              " 'управляющий': 839,\n",
              " 'результаты': 840,\n",
              " 'чего': 841,\n",
              " 'добавил': 842,\n",
              " 'плана': 843,\n",
              " 'месяц': 844,\n",
              " 'народный': 845,\n",
              " 'холдинга': 846,\n",
              " 'меньше': 847,\n",
              " 'которым': 848,\n",
              " 'совет': 849,\n",
              " 'общей': 850,\n",
              " 'иа': 851,\n",
              " 'эксперты': 852,\n",
              " 'истории': 853,\n",
              " 'кнб': 854,\n",
              " 'человека': 855,\n",
              " 'районе': 856,\n",
              " 'программу': 857,\n",
              " 'ниже': 858,\n",
              " 'позже': 859,\n",
              " 'задачи': 860,\n",
              " 'какие': 861,\n",
              " 'поездов': 862,\n",
              " 'частных': 863,\n",
              " 'рейтинг': 864,\n",
              " 'общество': 865,\n",
              " 'ответственности': 866,\n",
              " 'отношений': 867,\n",
              " 'алматинской': 868,\n",
              " 'защите': 869,\n",
              " 'бюджет': 870,\n",
              " 'имеют': 871,\n",
              " 'направлении': 872,\n",
              " 'такого': 873,\n",
              " 'мир': 874,\n",
              " 'реализацию': 875,\n",
              " 'председателем': 876,\n",
              " 'странами': 877,\n",
              " 'куандык': 878,\n",
              " 'экономических': 879,\n",
              " 'собой': 880,\n",
              " 'действия': 881,\n",
              " 'отмечается': 882,\n",
              " 'конкуренции': 883,\n",
              " 'банке': 884,\n",
              " 'увеличить': 885,\n",
              " 'использования': 886,\n",
              " 'работников': 887,\n",
              " 'компанией': 888,\n",
              " 'занимаемой': 889,\n",
              " 'начал': 890,\n",
              " 'нашего': 891,\n",
              " 'ничего': 892,\n",
              " 'основной': 893,\n",
              " 'наш': 894,\n",
              " 'евразийского': 895,\n",
              " 'первое': 896,\n",
              " '2007': 897,\n",
              " 'международный': 898,\n",
              " 'находятся': 899,\n",
              " 'народного': 900,\n",
              " 'социального': 901,\n",
              " 'ведомства': 902,\n",
              " 'регионе': 903,\n",
              " 'различных': 904,\n",
              " 'портфеля': 905,\n",
              " 'карты': 906,\n",
              " 'секторе': 907,\n",
              " 'холдинг': 908,\n",
              " 'пост': 909,\n",
              " 'своим': 910,\n",
              " 'пенсионного': 911,\n",
              " 'казахский': 912,\n",
              " 'общего': 913,\n",
              " 'право': 914,\n",
              " 'одна': 915,\n",
              " 'обеспечение': 916,\n",
              " 'показатель': 917,\n",
              " 'движения': 918,\n",
              " 'осуществляется': 919,\n",
              " 'годах': 920,\n",
              " 'сектор': 921,\n",
              " 'казахстанские': 922,\n",
              " 'такое': 923,\n",
              " 'клиентов': 924,\n",
              " 'пяти': 925,\n",
              " 'техники': 926,\n",
              " 'процесс': 927,\n",
              " 'говорить': 928,\n",
              " 'энергии': 929,\n",
              " 'легковых': 930,\n",
              " 'общественного': 931,\n",
              " 'член': 932,\n",
              " 'новая': 933,\n",
              " 'собственных': 934,\n",
              " 'kaznex': 935,\n",
              " 'мероприятий': 936,\n",
              " 'годом': 937,\n",
              " 'линии': 938,\n",
              " 'доли': 939,\n",
              " '90': 940,\n",
              " 'дороги': 941,\n",
              " 'себе': 942,\n",
              " 'выставки': 943,\n",
              " 'стаж': 944,\n",
              " 'генеральный': 945,\n",
              " 'даму': 946,\n",
              " 'жолы': 947,\n",
              " 'группа': 948,\n",
              " 'потребителей': 949,\n",
              " 'инвестиционного': 950,\n",
              " 'перевозки': 951,\n",
              " 'потенциал': 952,\n",
              " 'особо': 953,\n",
              " 'регионов': 954,\n",
              " 'регионах': 955,\n",
              " 'договора': 956,\n",
              " 'приходится': 957,\n",
              " 'сотрудников': 958,\n",
              " 'казахского': 959,\n",
              " 'размер': 960,\n",
              " 'состоянию': 961,\n",
              " '45': 962,\n",
              " 'расследование': 963,\n",
              " 'тенге1': 964,\n",
              " 'хорошо': 965,\n",
              " 'единый': 966,\n",
              " 'представитель': 967,\n",
              " 'правительством': 968,\n",
              " 'интересов': 969,\n",
              " 'вновь': 970,\n",
              " '300': 971,\n",
              " 'обязательств': 972,\n",
              " 'специалистов': 973,\n",
              " 'деле': 974,\n",
              " 'работают': 975,\n",
              " 'пояснил': 976,\n",
              " 'показатели': 977,\n",
              " 'жамбылской': 978,\n",
              " 'закон': 979,\n",
              " 'первым': 980,\n",
              " 'свободы': 981,\n",
              " 'проведение': 982,\n",
              " 'операций': 983,\n",
              " 'бву': 984,\n",
              " 'облигаций': 985,\n",
              " 'итоге': 986,\n",
              " 'таможенного': 987,\n",
              " 'места': 988,\n",
              " 'ноябре': 989,\n",
              " 'мировой': 990,\n",
              " 'тг': 991,\n",
              " 'содержания': 992,\n",
              " '70': 993,\n",
              " 'многих': 994,\n",
              " 'центральной': 995,\n",
              " 'ежегодно': 996,\n",
              " 'налогового': 997,\n",
              " 'вообще': 998,\n",
              " 'автомобиль': 999,\n",
              " 'государственные': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za00-UDWkVCi"
      },
      "source": [
        " df['encode'] =  df['text_lower']"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy5NBVUuit9j"
      },
      "source": [
        "##Токенизация — кодирование слов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDPnJIEUiZQb",
        "outputId": "3fb3b73d-8042-4e2d-85de-d0ed218e71d5"
      },
      "source": [
        "for i in range(len(df['text_lower'])):\n",
        "  df['encode'][i] = [vocab_to_int[w] for w in df['text_lower'][i]]"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgga8lj9lvfp",
        "outputId": "d9d87ebb-f35f-4f10-8b74-7b61b0cf9bb8"
      },
      "source": [
        "df['encode']"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [1868, 963, 5, 1155, 2270, 77, 4797, 985, 65, ...\n",
              "1       [11461, 1672, 18, 1614, 16113, 6811, 4, 350, 5...\n",
              "2       [1530, 393, 63, 20, 5509, 22939, 17106, 25276,...\n",
              "3       [5, 174, 221, 233, 29, 9, 4, 271, 37, 5565, 74...\n",
              "4       [107, 352, 190, 650, 1553, 3, 704, 31, 5, 779,...\n",
              "                              ...                        \n",
              "8258    [20, 26, 7461, 82, 5551, 10, 1387, 1729, 14, 2...\n",
              "8259    [33, 2728, 198, 99, 73, 438, 4729, 25239, 512,...\n",
              "8260    [3, 1, 77, 540, 313, 151, 138, 3, 10603, 876, ...\n",
              "8261    [1, 61, 40, 1561, 69, 1004, 7759, 71, 189950, ...\n",
              "8262    [761, 31, 816, 1800, 526, 303, 5872, 306, 10, ...\n",
              "Name: encode, Length: 8263, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS4gTDqfpO5k"
      },
      "source": [
        "##Кодирование меток"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "fEKQyM-lo0XJ",
        "outputId": "49387b2f-dff8-4ba4-ee21-fead9480dfa6"
      },
      "source": [
        "df"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text_lower</th>\n",
              "      <th>encode</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Досудебное расследование по факту покупки ЕНПФ...</td>\n",
              "      <td>1945</td>\n",
              "      <td>negative</td>\n",
              "      <td>[досудебное, расследование, по, факту, покупки...</td>\n",
              "      <td>[1868, 963, 5, 1155, 2270, 77, 4797, 985, 65, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Медики рассказали о состоянии пострадавшего му...</td>\n",
              "      <td>1957</td>\n",
              "      <td>negative</td>\n",
              "      <td>[медики, рассказали, о, состоянии, пострадавше...</td>\n",
              "      <td>[11461, 1672, 18, 1614, 16113, 6811, 4, 350, 5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Прошел почти год, как железнодорожным оператор...</td>\n",
              "      <td>1969</td>\n",
              "      <td>negative</td>\n",
              "      <td>[прошел, почти, год, как, железнодорожным, опе...</td>\n",
              "      <td>[1530, 393, 63, 20, 5509, 22939, 17106, 25276,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>По итогам 12 месяцев 2016 года на территории р...</td>\n",
              "      <td>1973</td>\n",
              "      <td>negative</td>\n",
              "      <td>[по, итогам, 12, месяцев, 2016, года, на, терр...</td>\n",
              "      <td>[5, 174, 221, 233, 29, 9, 4, 271, 37, 5565, 74...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Астана. 21 ноября. Kazakhstan Today - Агентств...</td>\n",
              "      <td>1975</td>\n",
              "      <td>negative</td>\n",
              "      <td>[астана, 21, ноября, kazakhstan, today, , аген...</td>\n",
              "      <td>[107, 352, 190, 650, 1553, 3, 704, 31, 5, 779,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8258</th>\n",
              "      <td>Как мы писали еще весной, для увеличения сбыта...</td>\n",
              "      <td>10312</td>\n",
              "      <td>positive</td>\n",
              "      <td>[как, мы, писали, еще, весной, для, увеличения...</td>\n",
              "      <td>[20, 26, 7461, 82, 5551, 10, 1387, 1729, 14, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8259</th>\n",
              "      <td>Но молодой министр национальной экономики Биши...</td>\n",
              "      <td>10313</td>\n",
              "      <td>negative</td>\n",
              "      <td>[но, молодой, министр, национальной, экономики...</td>\n",
              "      <td>[33, 2728, 198, 99, 73, 438, 4729, 25239, 512,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8260</th>\n",
              "      <td>\\n \\nВ ЕНПФ назначен новый председатель правле...</td>\n",
              "      <td>10314</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[, в, енпф, назначен, новый, председатель, пра...</td>\n",
              "      <td>[3, 1, 77, 540, 313, 151, 138, 3, 10603, 876, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8261</th>\n",
              "      <td>В Алматы у отделения банка произошло нападение...</td>\n",
              "      <td>10315</td>\n",
              "      <td>negative</td>\n",
              "      <td>[в, алматы, у, отделения, банка, произошло, на...</td>\n",
              "      <td>[1, 61, 40, 1561, 69, 1004, 7759, 71, 189950, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8262</th>\n",
              "      <td>НПП РК «Атамекен» предлагает создать Националь...</td>\n",
              "      <td>10316</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[нпп, рк, атамекен, предлагает, создать, нацио...</td>\n",
              "      <td>[761, 31, 816, 1800, 526, 303, 5872, 306, 10, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8263 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  ...                                             encode\n",
              "0     Досудебное расследование по факту покупки ЕНПФ...  ...  [1868, 963, 5, 1155, 2270, 77, 4797, 985, 65, ...\n",
              "1     Медики рассказали о состоянии пострадавшего му...  ...  [11461, 1672, 18, 1614, 16113, 6811, 4, 350, 5...\n",
              "2     Прошел почти год, как железнодорожным оператор...  ...  [1530, 393, 63, 20, 5509, 22939, 17106, 25276,...\n",
              "3     По итогам 12 месяцев 2016 года на территории р...  ...  [5, 174, 221, 233, 29, 9, 4, 271, 37, 5565, 74...\n",
              "4     Астана. 21 ноября. Kazakhstan Today - Агентств...  ...  [107, 352, 190, 650, 1553, 3, 704, 31, 5, 779,...\n",
              "...                                                 ...  ...                                                ...\n",
              "8258  Как мы писали еще весной, для увеличения сбыта...  ...  [20, 26, 7461, 82, 5551, 10, 1387, 1729, 14, 2...\n",
              "8259  Но молодой министр национальной экономики Биши...  ...  [33, 2728, 198, 99, 73, 438, 4729, 25239, 512,...\n",
              "8260  \\n \\nВ ЕНПФ назначен новый председатель правле...  ...  [3, 1, 77, 540, 313, 151, 138, 3, 10603, 876, ...\n",
              "8261  В Алматы у отделения банка произошло нападение...  ...  [1, 61, 40, 1561, 69, 1004, 7759, 71, 189950, ...\n",
              "8262  НПП РК «Атамекен» предлагает создать Националь...  ...  [761, 31, 816, 1800, 526, 303, 5872, 306, 10, ...\n",
              "\n",
              "[8263 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2kzjKhntbIJ"
      },
      "source": [
        "##Заполнение и усечение новостей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7FlkdgSpB8L",
        "outputId": "ef29196f-42aa-4fc6-e181-ac3745170619"
      },
      "source": [
        "df['sentiment'].unique()"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['negative', 'positive', 'neutral'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt6ymqRHpNiR"
      },
      "source": [
        "df['label'] = df['sentiment']"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4A9FRbNpxII",
        "outputId": "def538e6-91a8-418d-8fd4-8da248df09b1"
      },
      "source": [
        "for i in range(len(df['sentiment'])):\n",
        "  if df['sentiment'][i] == 'negative':\n",
        "    df['label'][i] = -1\n",
        "  elif df['sentiment'][i] == 'neutral':\n",
        "    df['label'][i] = 0\n",
        "  else:\n",
        "    df['label'][i] = 1"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "77WR5Nvyqh2w",
        "outputId": "ca4ba7e2-bb85-46f9-9c4b-4610bee31499"
      },
      "source": [
        "df.groupby('sentiment').count()"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>id</th>\n",
              "      <th>text_lower</th>\n",
              "      <th>encode</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>1434</td>\n",
              "      <td>1434</td>\n",
              "      <td>1434</td>\n",
              "      <td>1434</td>\n",
              "      <td>1434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>4034</td>\n",
              "      <td>4034</td>\n",
              "      <td>4034</td>\n",
              "      <td>4034</td>\n",
              "      <td>4034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>2795</td>\n",
              "      <td>2795</td>\n",
              "      <td>2795</td>\n",
              "      <td>2795</td>\n",
              "      <td>2795</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           text    id  text_lower  encode  label\n",
              "sentiment                                       \n",
              "negative   1434  1434        1434    1434   1434\n",
              "neutral    4034  4034        4034    4034   4034\n",
              "positive   2795  2795        2795    2795   2795"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "h4XUGNkZsMJy",
        "outputId": "9dffe68d-d7e7-4d99-cee3-27f3e14d29d1"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "news_len = [len(x) for x in df['encode']]\n",
        "pd.Series(news_len).hist(bins=100)\n",
        "plt.show()\n",
        "pd.Series(news_len).describe()"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     8263.000000\n",
              "mean       515.478519\n",
              "std       1192.447019\n",
              "min          4.000000\n",
              "25%        165.000000\n",
              "50%        273.000000\n",
              "75%        484.000000\n",
              "max      49039.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58wl5ClFvT5S",
        "outputId": "4153f347-10f7-4078-806e-de10eb46495b"
      },
      "source": [
        "df['encode']"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [1868, 963, 5, 1155, 2270, 77, 4797, 985, 65, ...\n",
              "1       [11461, 1672, 18, 1614, 16113, 6811, 4, 350, 5...\n",
              "2       [1530, 393, 63, 20, 5509, 22939, 17106, 25276,...\n",
              "3       [5, 174, 221, 233, 29, 9, 4, 271, 37, 5565, 74...\n",
              "4       [107, 352, 190, 650, 1553, 3, 704, 31, 5, 779,...\n",
              "                              ...                        \n",
              "8258    [20, 26, 7461, 82, 5551, 10, 1387, 1729, 14, 2...\n",
              "8259    [33, 2728, 198, 99, 73, 438, 4729, 25239, 512,...\n",
              "8260    [3, 1, 77, 540, 313, 151, 138, 3, 10603, 876, ...\n",
              "8261    [1, 61, 40, 1561, 69, 1004, 7759, 71, 189950, ...\n",
              "8262    [761, 31, 816, 1800, 526, 303, 5872, 306, 10, ...\n",
              "Name: encode, Length: 8263, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW9g8rYkw3kp"
      },
      "source": [
        "seq_length = 515"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnxfKLrdxql5"
      },
      "source": [
        "features = np.zeros((len(df['encode']), seq_length), dtype = int)\n",
        "for i, news in enumerate(df['encode']):\n",
        "  news_len = len(news)\n",
        "  if news_len < 1707:\n",
        "    zeroes = list(np.zeros(seq_length-news_len))\n",
        "    new = zeroes+news\n",
        "  elif news_len > seq_length:\n",
        "    new = review[0:seq_length]      \n",
        "  features[i,:] = np.array(new)\n",
        "\n"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogsUIQ9YxYZ6"
      },
      "source": [
        "def pad_features(pdseries, seq_length):\n",
        "  features = np.zeros((len(df['encode']), seq_length), dtype = int)\n",
        "  for i, news in enumerate(df['encode']):\n",
        "    news_len = len(news)\n",
        "    if news_len < seq_length:\n",
        "      zeroes = list(np.zeros(seq_length-news_len))\n",
        "      new = zeroes+news\n",
        "    elif news_len > seq_length:\n",
        "      new = news[0:seq_length]      \n",
        "    features[i,:] = np.array(new)\n",
        "  return features"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsYy7DXl3mQx"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMgSGUqU0bV6"
      },
      "source": [
        "features = pad_features(df['encode'], seq_length)\n",
        "len_feat = len(features)"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLSLfqnt3B0I"
      },
      "source": [
        "label = df['label'].to_numpy()"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grECJzYE36qi",
        "outputId": "8771359b-824d-417b-f6f3-bc613254160c"
      },
      "source": [
        "type(label)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp5Ky-_b01Ji"
      },
      "source": [
        "split_frac = 0.8\n",
        "train_x = features[0:int(split_frac*len_feat)]\n",
        "train_y = label[0:int(split_frac*len_feat)]\n",
        "remaining_x = features[int(split_frac*len_feat):]\n",
        "remaining_y = label[int(split_frac*len_feat):]\n",
        "valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
        "valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
        "test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
        "test_y = remaining_y[int(len(remaining_y)*0.5):]"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r0YLjId2wkY"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU0qXl5o20g7",
        "outputId": "99ba85a6-1156-4e5e-fa1e-688db0702992"
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 515])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ...,     6,  2724,   660],\n",
            "        [    0,     0,     0,  ...,     3,   620, 88564],\n",
            "        [    0,     0,     0,  ...,  1880,   620,  1275],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,   459,   353,  1324],\n",
            "        [42036,   620, 23380,  ...,     7,    10,  2045],\n",
            "        [    0,     0,     0,  ...,     3,     3,     3]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([-1,  1,  0, -1,  0,  1,  1, -1,  0,  0,  1,  1,  0,  1, -1,  0, -1,  0,\n",
            "         0,  0,  1,  1, -1,  0,  1,  1,  0,  0, -1,  0,  0,  0, -1,  0,  0,  1,\n",
            "         0,  0, -1,  1,  0, -1,  0, -1,  0,  0,  0,  0, -1,  1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl5EgYaB4EuY"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p70LBFrP4Kc4",
        "outputId": "87dc8886-0c10-4c45-ac51-896e33a807cf"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(189952, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvID33hz4eMq"
      },
      "source": [
        "train_on_gpu = True\n"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "Oz1jcd-O4QvR",
        "outputId": "37b15a74-b791-439c-8f28-1763fe1130ce"
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "# training params\n",
        "\n",
        "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        inputs = inputs.type(torch.LongTensor)\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                inputs = inputs.type(torch.LongTensor)\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-236-edd2e9938a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# move model to GPU, if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_on_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    }
  ]
}